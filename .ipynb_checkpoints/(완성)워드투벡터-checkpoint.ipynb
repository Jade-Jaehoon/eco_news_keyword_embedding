{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU  사용 가능\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback ,EarlyStopping\n",
    "\n",
    "import pathlib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import neptune\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "print(\"GPU \", \"사용 가능\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"사용 불가능\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeptuneLogger(Callback):\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        for log_name, log_value in logs.items():\n",
    "            neptune.log_metric(f'batch_{log_name}', log_value)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        for log_name, log_value in logs.items():\n",
    "            neptune.log_metric(f'epoch_{log_name}', log_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen(x):\n",
    "    sentences = []\n",
    "    sentences.append(x.split(','))\n",
    "    return sentences[0]\n",
    "\n",
    "def remove_values_from_list(the_list,val):\n",
    "    return [value for value in the_list if value != val]   \n",
    "\n",
    "def raw_data(file,vals):\n",
    "    raw_data=pd.read_excel(file)\n",
    "    raw_data['기관']=raw_data['기관'].apply(lambda x: sen(x))\n",
    "    for val in vals:\n",
    "        raw_data['기관']=raw_data['기관'].map(lambda x: remove_values_from_list(x,val))\n",
    "    raw_data['기관1']=[','.join(map(str, l)) for l in raw_data['기관']]\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_word2int(listseries):# str을 넣어서 매핑할 워드int만들기\n",
    "    word2int = {}\n",
    "    \n",
    "    words = listseries.str.cat(sep=', ')\n",
    "    words = words.split(\",\")\n",
    "    words = [x.strip(' ') for x in words] #빈칸지우기\n",
    "    words =  list(filter(None, words)) \n",
    "    words =  list(set(words))   #빈칸지우고 유일한것만 남기기\n",
    "    for i,word in enumerate(words):\n",
    "        word2int[word] = i\n",
    "        \n",
    "    return words ,word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngram(listseries,WINDOW_SIZE): #리스트로 된 pandas.core.series.Series\n",
    "    data = []\n",
    "    for sentence in listseries: #시리즈에 있는 하나의 라인마다\n",
    "        for idx,word in enumerate(sentence): #인덱스랑 단어를 꺼내서\n",
    "            for neighbor in sentence[max(idx - WINDOW_SIZE ,0) : min( idx+ WINDOW_SIZE, len(sentence))]:\n",
    "                if neighbor != word:\n",
    "                    data.append([word,neighbor])\n",
    "    \n",
    "    df=pd.DataFrame(data, columns = ['input','label'])\n",
    "    df['input'] = df['input'].map(word2int) \n",
    "    df['label'] = df['label'].map(word2int) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairset(data):\n",
    "    pairs = [tuple(x) for x in data.to_numpy()] #튜플리스트만들기\n",
    "    pairs_set = set(pairs)\n",
    "    return pairs,pairs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1, classification = False):\n",
    "    \"\"\"Generate batches of samples for training\"\"\"\n",
    "    #배치사이즈 \n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    #배치사이즈 x 3 의 batch만들기  batch를 저장할 numpy 배열을 준비합니다.\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    # Adjust label based on task #분류문제면 1,0,지금은 1-1\n",
    "    if classification:\n",
    "        neg_label = 0\n",
    "    else:\n",
    "        neg_label = -1\n",
    "    \n",
    "    # This creates a generator\n",
    "    while True:\n",
    "        # randomly choose positive examples 긍정라벨갯수만큼 뽑음 랜덤으로 True인 샘플을 준비합니다.\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "\n",
    "        # Increment idx by 1\n",
    "        idx += 1\n",
    "        \n",
    "        # Add negative examples until reach batch size 부정라벨은 총 배치사이즈까지 뽑음\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # random selection 임의로 뽑아서\n",
    "            random_book = random.randrange(len(words))\n",
    "            random_link = random.randrange(len(words))\n",
    "            \n",
    "            # Check to make sure this is not a positive example 페어셋에 있는지 확인하고\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index 배치에 추가함\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        # Make sure to shuffle order 배치에 저장된 데이터들의 순서를 섞습니다\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'company1': batch[:, 0], 'company2': batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_embedding_model(OPTIMIZER='Adam', embedding_size = 100, classification = False ):\n",
    "    \"\"\"Model to embed books and wikilinks using the functional API.\n",
    "       Trained to discern if a link is present in a article\"\"\"\n",
    "    \n",
    "    # Both inputs are 1-dimensional\n",
    "    company1 = Input(name = 'company1', shape = [1])\n",
    "    company2 = Input(name = 'company2', shape = [1])\n",
    "    \n",
    "    # Embedding the book (shape will be (None, 1, 50))\n",
    "    company1_embedding =Embedding(name = 'company1_embedding',\n",
    "                                           input_dim = len(words),\n",
    "                                           output_dim = embedding_size)(company1)\n",
    "    \n",
    "    # Embedding the link (shape will be (None, 1, 50))\n",
    "    company2_embedding =Embedding(name = 'company2_embedding',\n",
    "                                                    input_dim = len(words),\n",
    "                                                    output_dim = embedding_size)(company2)\n",
    "    \n",
    "    # Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n",
    "    merged = Dot(name = 'dot_product', normalize = True, axes = 2)([company1_embedding, company2_embedding])\n",
    "    \n",
    "    # Reshape to be a single number (shape will be (None, 1))\n",
    "    merged = Reshape(target_shape = [1])(merged)\n",
    "    \n",
    "    # If classifcation, add extra layer and loss function is binary cross entropy\n",
    "    if classification:\n",
    "        merged = Dense(1, activation = 'sigmoid')(merged)\n",
    "        model = Model(inputs = [company1, company2], outputs = merged)\n",
    "        model.compile(optimizer = OPTIMIZER , loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Otherwise loss function is mean squared error\n",
    "    else:\n",
    "        model = Model(inputs = [company1, company2], outputs = merged)\n",
    "        model.compile(optimizer = OPTIMIZER, loss = 'mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 파일읽고 단어 지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file ='embemb.xlsx'\n",
    "vals =['한국거래소' ,'정부','유가증권', 'LP','금융위원회','금융감독원','한경로보', '대신증권' ,'코스닥','씽크풀','한국경제신문','대상','후보추천위원회','프랑스령 기아나' ]\n",
    "raw_data=raw_data(file,vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 윈도우 사이즈 지정하고 학습데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "WINDOW_SIZE = 20\n",
    "\n",
    "words,word2int   = make_word2int(raw_data['기관1'])\n",
    "data             = make_ngram   (raw_data['기관' ],WINDOW_SIZE)\n",
    "pairs,pairs_set  = make_pairset (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩 차원과 옵티마이져 배치사이즈 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE=250\n",
    "OPTIMIZER='Adam'\n",
    "N_POSITIVE=32768 # x값 \n",
    "NEGATIVE_RATIO=1 # y값 \n",
    "# x(1+y) 만큼이 총배치사이즈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파라미터 전체지정후 넵튠켜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주소\n",
    "WHERE = 'sgeconomics/natural'\n",
    "#모델이름\n",
    "MODEL_NAME = 'init'\n",
    "#토큰\n",
    "TOKEN      = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiMzEyODExZTItMTJkOC00Mjk5LTgxNzItMjQwMGNjZGYwMGZmIn0='\n",
    "#저장할파라미터\n",
    "PARAMS ={\n",
    "        'window size' : WINDOW_SIZE,\n",
    "        'embedding dim' : EMBEDDING_SIZE,\n",
    "        'Optimizer' : OPTIMIZER,\n",
    "        'positive' :  N_POSITIVE,\n",
    "        'negative ratio' : NEGATIVE_RATIO\n",
    "        }\n",
    "#태그\n",
    "TAG=['adv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: It is not secure to place API token in your source code. You should treat it as a password to your account. It is strongly recommended to use NEPTUNE_API_TOKEN environment variable instead. Remember not to upload source file with API token to any public repository.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/sgeconomics/natural/e/NAT-6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(NAT-6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neptune.init(WHERE,api_token = TOKEN)\n",
    "neptune.create_experiment(name=MODEL_NAME,params =PARAMS, tags=TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 에포크 지정하고 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH =1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 300 steps\n",
      "Epoch 1/1000\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 0.6763\n",
      "Epoch 2/1000\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.2951\n",
      "Epoch 3/1000\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 0.2440\n",
      "Epoch 4/1000\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 0.2389\n",
      "Epoch 5/1000\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 0.2371\n",
      "Epoch 6/1000\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.2365\n",
      "Epoch 7/1000\n",
      "300/300 [==============================] - 89s 298ms/step - loss: 0.2360\n",
      "Epoch 8/1000\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 0.2359\n",
      "Epoch 9/1000\n",
      "300/300 [==============================] - 89s 298ms/step - loss: 0.2354\n",
      "Epoch 10/1000\n",
      "300/300 [==============================] - 90s 299ms/step - loss: 0.2353\n",
      "Epoch 11/1000\n",
      "300/300 [==============================] - 90s 298ms/step - loss: 0.2350\n",
      "Epoch 12/1000\n",
      "300/300 [==============================] - 89s 297ms/step - loss: 0.2350\n",
      "Epoch 13/1000\n",
      "300/300 [==============================] - 89s 296ms/step - loss: 0.2346\n",
      "Epoch 14/1000\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.2348\n",
      "Epoch 15/1000\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 0.2344\n",
      "Epoch 16/1000\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.2344\n",
      "Epoch 17/1000\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.2342\n",
      "Epoch 18/1000\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 0.2342\n",
      "Epoch 19/1000\n",
      "300/300 [==============================] - 92s 306ms/step - loss: 0.2339\n",
      "Epoch 20/1000\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 0.2338\n",
      "Epoch 21/1000\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 0.2342\n",
      "Epoch 22/1000\n",
      "300/300 [==============================] - 90s 302ms/step - loss: 0.2337\n",
      "Epoch 23/1000\n",
      "300/300 [==============================] - 92s 307ms/step - loss: 0.2338\n",
      "Epoch 24/1000\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 0.2337\n",
      "Epoch 25/1000\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.2338\n",
      "Epoch 26/1000\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 0.2337\n",
      "Epoch 27/1000\n",
      "300/300 [==============================] - 90s 300ms/step - loss: 0.2335\n",
      "Epoch 28/1000\n",
      "300/300 [==============================] - 91s 302ms/step - loss: 0.2334\n",
      "Epoch 29/1000\n",
      "300/300 [==============================] - 90s 300ms/step - loss: 0.2335\n",
      "Epoch 30/1000\n",
      "300/300 [==============================] - 93s 310ms/step - loss: 0.2333\n",
      "Epoch 31/1000\n",
      "300/300 [==============================] - 96s 318ms/step - loss: 0.2333\n",
      "Epoch 32/1000\n",
      "300/300 [==============================] - 94s 314ms/step - loss: 0.2332\n",
      "Epoch 33/1000\n",
      "300/300 [==============================] - 95s 317ms/step - loss: 0.2332\n",
      "Epoch 34/1000\n",
      "300/300 [==============================] - 95s 316ms/step - loss: 0.2331\n",
      "Epoch 35/1000\n",
      "300/300 [==============================] - 94s 313ms/step - loss: 0.2332\n",
      "Epoch 36/1000\n",
      "300/300 [==============================] - 92s 305ms/step - loss: 0.2331\n",
      "Epoch 37/1000\n",
      "300/300 [==============================] - 88s 294ms/step - loss: 0.2330\n",
      "Epoch 38/1000\n",
      "300/300 [==============================] - 89s 297ms/step - loss: 0.2327\n",
      "Epoch 39/1000\n",
      "300/300 [==============================] - 90s 299ms/step - loss: 0.2330\n",
      "Epoch 40/1000\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.2329\n",
      "Epoch 41/1000\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.2328\n",
      "Epoch 42/1000\n",
      "300/300 [==============================] - 90s 301ms/step - loss: 0.2329\n",
      "Epoch 43/1000\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.2328\n",
      "Epoch 44/1000\n",
      "300/300 [==============================] - 91s 304ms/step - loss: 0.2328\n",
      "Epoch 45/1000\n",
      "300/300 [==============================] - 91s 303ms/step - loss: 0.2325\n",
      "Epoch 46/1000\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 0.2327\n",
      "Epoch 47/1000\n",
      "300/300 [==============================] - 93s 309ms/step - loss: 0.2326\n",
      "Epoch 48/1000\n",
      "300/300 [==============================] - 94s 313ms/step - loss: 0.2326\n",
      "Epoch 49/1000\n",
      "300/300 [==============================] - 95s 315ms/step - loss: 0.2326\n",
      "Epoch 50/1000\n",
      "278/300 [==========================>...] - ETA: 6s - loss: 0.2325"
     ]
    }
   ],
   "source": [
    "model = book_embedding_model(OPTIMIZER,EMBEDDING_SIZE)\n",
    "gen = generate_batch(pairs, n_positive=N_POSITIVE ,negative_ratio=NEGATIVE_RATIO)\n",
    "h = model.fit(gen,\n",
    "              epochs = EPOCH, \n",
    "              steps_per_epoch = len(pairs) // N_POSITIVE,\n",
    "             # callbacks=[NeptuneLogger()],\n",
    "              verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model.h5')\n",
    "neptune.log_artifact('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "company1_layer = model.get_layer('company1_embedding')\n",
    "company1_weights = company1_layer.get_weights()[0]\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for key,value in word2int.items():\n",
    "  vec = company1_weights[value] # skip 0, it's padding.\n",
    "  out_m.write(key + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "neptune.log_artifact('vecs.tsv')\n",
    "neptune.log_artifact('meta.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 넵튠 추적 종료하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
